{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM with word2vec embeddings</h1><br>\n",
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ok, I see I need to download the data files and arrange them. Best do that now."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I downloaded the kaggle files and unzipped them. Now to creat the \"input\" dir as called above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_kaggle_download_zip_files    sample_submission.csv  train.csv\r\n",
      "LSTM with word2vec embeddings.ipynb  test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jesus! This site: https://github.com/andrewdblevins/beyond_word2vec tells me that I'm about to download a 3Gb+ file 'GoogleNews-vectors-negative300.bin'<br>\n",
    "<h3>Download the pretrained google news word vectors</h3><br>\n",
    "<b><i>warning: This file is 3.6GB</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-30 11:06:11--  https://s3.amazonaws.com/mordecai-geo/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.18.251\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.18.251|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  20.2MB/s    in 89s     \n",
      "\n",
      "2017-11-30 11:07:41 (17.6 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/mordecai-geo/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I need to check the amount of space left on the drive!\n",
    "Note - I checked in Cygwin while I was waiting and it looks like it'll be fine! But I'll re-run the check here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "udev             30G     0   30G   0% /dev\r\n",
      "tmpfs           6.0G  8.8M  6.0G   1% /run\r\n",
      "/dev/xvda1      126G   13G  108G  11% /\r\n",
      "tmpfs            30G     0   30G   0% /dev/shm\r\n",
      "tmpfs           5.0M     0  5.0M   0% /run/lock\r\n",
      "tmpfs            30G     0   30G   0% /sys/fs/cgroup\r\n",
      "tmpfs           6.0G   12K  6.0G   1% /run/user/1000\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip the gz file\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin   sample_submission.csv\r\n",
      "\u001b[0m\u001b[01;34minitial_kaggle_download_zip_files\u001b[0m/   test.csv\r\n",
      "\u001b[01;34minput\u001b[0m/                               train.csv\r\n",
      "LSTM with word2vec embeddings.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ha! gunzip deletes a gz file after it's been unzipped! I didn't know that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next step - move the train and test csv files into the \"input\" folder\n",
    "!mv *.csv input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin  \u001b[0m\u001b[01;34minput\u001b[0m/\r\n",
      "\u001b[01;34minitial_kaggle_download_zip_files\u001b[0m/  LSTM with word2vec embeddings.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move the bin file into the input folder too\n",
    "mv *.bin input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_kaggle_download_zip_files  LSTM with word2vec embeddings-backup01.ipynb\r\n",
      "input\t\t\t\t   LSTM with word2vec embeddings.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin  sample_submission.csv  test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Restarting the script in chunks from the start</H1>\n",
    "Because the above was basically setting it up...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Single model may achieve LB scores at around 0.29+ ~ 0.30+\n",
    "Average ensembles can easily get 0.28+ or less\n",
    "Don't need to be an expert of feature engineering\n",
    "All you need is a GPU!!!!!!!\n",
    "\n",
    "The code is tested on Keras 2.0.0 using Tensorflow backend, and Python 2.7\n",
    "\n",
    "According to experiments by kagglers, Theano backend with GPU may give bad LB scores while\n",
    "        the val_loss seems to be fine, so try Tensorflow backend first please\n",
    "'''\n",
    "\n",
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = 'input/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "                            #GoogleNews-vectors-negative300.bin\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Going to copy it verbatim and see how it runs. But copying it in chunks to manage errors better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
