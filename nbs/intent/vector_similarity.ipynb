{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intent detection with vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code based on https://www.kaggle.com/tj2552/similarity-techniques-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../data/\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of base training File = ', (20000, 6))\n",
      "('Shape of base training data after cleaning = ', (20000, 6))\n",
      "('Shape of test data = ', (400, 6))\n",
      "\n",
      "training data head\n",
      "------------------\n",
      "        id  qid1  qid2                                          question1  \\\n",
      "88      88   177   178     Which is the best gaming laptop under 60k INR?   \n",
      "1482  1482  2951  2952  What is more beneficial to learn, SolidWorks o...   \n",
      "\n",
      "                                              question2  is_duplicate  \n",
      "88      Which is the best gaming laptop under Rs 60000?             1  \n",
      "1482  Are softwares like SolidWorks, CATIA and Pro E...             0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"data/train.csv\", nrows=20000)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    # Remove missing values and duplicates from training data\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "\n",
    "df = read_data()\n",
    "df_train, df_test = train_test_split(df, test_size = 0.02)\n",
    "print ('Shape of test data = ', df_test.shape)\n",
    "print\n",
    "print 'training data head'\n",
    "print '------------------'\n",
    "print (df_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Exploratory Data Analysis (EDA) on the data to get a look and feel about the data. Here we are trying to see the distribution of output data. Duplicate questions available etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eda(df):\n",
    "    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n",
    "           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n",
    "    \n",
    "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
    "    \n",
    "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
    "    \n",
    "    question_ids_counter = Counter(question_ids_combined)\n",
    "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
    "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
    "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Count = 7334 , Non Duplicate Count = 12266\n",
      "Unique Questions = 37045\n",
      "Count of Quesitons appearing more than once = 1756\n"
     ]
    }
   ],
   "source": [
    "eda(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/anaconda2/lib/python2.7/site-packages/nltk']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re                           # regular expressions\n",
    "import gensim                       # vector space modeling and topic modeling toolkit \n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords   # natural language toolkit\n",
    "from nltk.stem.porter import *\n",
    "nltk.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words in the dictionary = 4823\n"
     ]
    }
   ],
   "source": [
    "df_train = tokenize_questions(df_train)\n",
    "dictionary = train_dictionary(df_train)\n",
    "print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "df_test = tokenize_questions(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create sparse vector matrices\n",
    "Use Bag Of Words Technique to convert sentences into vectors. \n",
    "\n",
    "There are two vector matrices thus created where each of the matrix is a sparse matrix to save memory in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
    "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
    "    \n",
    "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
    "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return question1_csc.transpose(),question2_csc.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size of the training data) X (no of words in the dictionary)\n",
      "-------------------------------------------------------------\n",
      "('q1_csc shape', (19600, 4823))\n",
      "('q2_csc shape', (19600, 4823))\n"
     ]
    }
   ],
   "source": [
    "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
    "print '(size of the training data) X (no of words in the dictionary)'\n",
    "print '-------------------------------------------------------------'\n",
    "print ('q1_csc shape', q1_csc.shape)\n",
    "print ('q2_csc shape', q2_csc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
